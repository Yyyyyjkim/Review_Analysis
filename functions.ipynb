{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPimjzC+S6q35Cr+DofUy1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yyyyyjkim/review_analysis/blob/master/functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDrdAkNeUaSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install requests\n",
        "!pip install bs4\n",
        "!pip install selenium\n",
        "!pip install konlpy\n",
        "!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpp3lOBUanX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "from konlpy.tag import Okt\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "import dill\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MCp_41NNk7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class crawling:\n",
        "  def __init__(self):\n",
        "    self.options = webdriver.ChromeOptions()\n",
        "    self.options.add_argument('--headless')\n",
        "    self.options.add_argument('--no-sandbox')\n",
        "    self.options.add_argument('--disable-dev-shm-usage')\n",
        "    self.driver = webdriver.Chrome('chromedriver',options=self.options)\n",
        "  \n",
        "  def get_text(self,url,sorting,pagenum):\n",
        "    # go to review page and sorted by sorting\n",
        "    self.driver.get(url)\n",
        "    self.driver.find_element_by_xpath(\"//div[@class='detail_tab_floatable']//li[@class='item']/a[@class='link N=a:tab.review']\").click()\n",
        "    time.sleep(5)\n",
        "    div_class = 'header_review _review_list_header'\n",
        "    ul_class = 'sort_list _review_list_header_sort'\n",
        "    path = f\"//div[@class='{div_class}']//ul[@class='{ul_class}']//a[text()='{sorting}']\"\n",
        "    self.driver.find_element_by_xpath(path).click()\n",
        "    time.sleep(5)\n",
        "\n",
        "    # get text from 1 to pagenum\n",
        "    html = self.driver.page_source\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "    review_page = soup.find_all('p',class_='review_text _review_text')\n",
        "    review = [each_line.get_text().strip() for each_line in review_page[:20]]\n",
        "    for page in range(2,(pagenum+1)):\n",
        "      if (page)%10 == 1:\n",
        "        path_page = f\"//nav[@class='module_pagination _review_list_page']//a[text()='Îã§Ïùå']\"\n",
        "        self.driver.find_element_by_xpath(path_page).click()\n",
        "      else:\n",
        "        path_page = f\"//nav[@class='module_pagination _review_list_page']//a[text()='{page}']\"\n",
        "        self.driver.find_element_by_xpath(path_page).click()\n",
        "      time.sleep(5)\n",
        "      html = self.driver.page_source\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      review_page = soup.find_all('p',class_='review_text _review_text')\n",
        "      review_page = [each_line.get_text().strip() for each_line in review_page[:20]]\n",
        "      review.extend(review_page)\n",
        "    \n",
        "    return review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0VX5X-OOQjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class nlp:\n",
        "  def __init__(self,text):\n",
        "    self.twitter = Okt()\n",
        "    self.tidy = [re.sub('[\\[\\]!\"#$%&\\'()*+,/:;<=>?@\\^_`{|}~-‚úîÔ∏èüôèüèªüëçü§óüíïüòò]|BEST\\n|Ïû¨Íµ¨Îß§\\n|ÌïúÎã¨ÏÇ¨Ïö©Í∏∞\\n','',sent) for sent in text]\n",
        "    self.tidy = [re.sub('\\n',' ',sent) for sent in self.tidy]\n",
        "\n",
        "  def get_morphs(self):\n",
        "    pos_text = [self.twitter.morphs(sent) for sent in self.tidy]\n",
        "    corpus = [\" \".join(word) for word in pos_text]\n",
        "    corpus = list(filter(lambda x: len(x)!=0, corpus))\n",
        "    return(corpus)\n",
        "\n",
        "  def get_pos(self):\n",
        "    pos_text = [self.twitter.pos(sent) for sent in self.tidy]\n",
        "    pos_text = [[word for word, tag in sent if tag == 'Noun' or tag=='Verb' or tag=='Adjective'] for sent in pos_text]\n",
        "    corpus = [\" \".join(word) for word in pos_text]\n",
        "    corpus = list(filter(lambda x: len(x)!=0, corpus))\n",
        "    return(corpus)\n",
        "\n",
        "  def get_noun(self):\n",
        "    pos_text = [self.twitter.nouns(sent) for sent in self.tidy]\n",
        "    corpus = [\" \".join(word) for word in pos_text]\n",
        "    corpus = list(filter(lambda x: len(x)!=0, corpus))\n",
        "    return(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJToywXEOZUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class graph:\n",
        "  def __init__(self):\n",
        "    self.tfidf = TfidfVectorizer()\n",
        "    self.counter = CountVectorizer()\n",
        "\n",
        "  def graph_sent(self,corpus):\n",
        "    mat_tfidf = self.tfidf.fit_transform(corpus).toarray()\n",
        "    graph_tfidf = np.dot(mat_tfidf,mat_tfidf.T)\n",
        "    return graph_tfidf\n",
        "\n",
        "  def graph_word(self,corpus):\n",
        "    mat_count = normalize(self.counter.fit_transform(corpus).toarray().astype(float),axis=0) #Ïôú normalize ÌïòÏßÄ?\n",
        "    vocab = self.counter.vocabulary_ # word:count\n",
        "    graph_word = np.dot(mat_count.T,mat_count)\n",
        "    words = {vocab[word]:word for word in vocab} # count:word\n",
        "    return graph_word, words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAThMY1fOd4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textrank():\n",
        "  def __init__(self,url,sorting,pagenum,unit='self'):\n",
        "    self.crawling = crawling()\n",
        "    self.text = self.crawling.get_text(url,sorting,pagenum)\n",
        "\n",
        "    self.nlp = nlp(self.text)\n",
        "    self.tidy = self.nlp.tidy\n",
        "    self.morphs = self.nlp.get_morphs()\n",
        "    self.pos = self.nlp.get_pos()\n",
        "    self.noun = self.nlp.get_noun()\n",
        "\n",
        "    self.graph = graph()\n",
        "    if unit=='self':\n",
        "      self.graph_sent = self.graph.graph_sent(self.morphs)\n",
        "      self.graph_word, self.words = self.graph.graph_word(self.morphs)\n",
        "    if unit=='pos':\n",
        "      self.graph_sent = self.graph.graph_sent(self.pos)\n",
        "      self.graph_word, self.words = self.graph.graph_word(self.pos)\n",
        "    if unit=='noun':\n",
        "      self.graph_sent = self.graph.graph_sent(self.noun)\n",
        "      self.graph_word, self.words = self.graph.graph_word(self.noun)\n",
        "    \n",
        "  def get_rank(self,graph='self',corpus='self',d=0.85,tol=1e-5,len=10):\n",
        "    if graph=='self':\n",
        "      graph = self.graph_sent\n",
        "    if corpus=='self':\n",
        "      corpus = self.tidy\n",
        "    colsum = np.sum(graph,axis=0)\n",
        "    weight = pd.DataFrame(graph).apply(lambda x:x/colsum,axis=1)\n",
        "    weight = weight.fillna(0)\n",
        "    pro_vec = np.ones(graph.shape[0]).reshape(-1,1)\n",
        "\n",
        "    bias = (1-d)*pro_vec\n",
        "    diff = sum(pro_vec)\n",
        "    while diff > tol:\n",
        "      past_vec = pro_vec.copy()\n",
        "      pro_vec = bias+d*np.dot(weight,pro_vec)\n",
        "      diff = sum(abs(past_vec-pro_vec))\n",
        "    \n",
        "    rank = {id: rank for id, rank in enumerate(pro_vec)}\n",
        "    keywords = sorted(rank, key=lambda x: rank[x],reverse=True) #valueÎ•º Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ†¨ÌïòÍ≥† key return \n",
        "\n",
        "    summary = []\n",
        "    for id in keywords[:len]:\n",
        "      summary.append(corpus[id])\n",
        "    \n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}